{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNuGXtPO6AQU3znpxVuoj0L",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/korupolujayanth2004/DeepLearning/blob/main/House__Price_Prediction_using_Multi_layer_perceptron.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#dataset\n",
        "#https://www.kaggle.com/competitions/house-prices-advanced-regression-techniques/data"
      ],
      "metadata": {
        "id": "dqjTsxice69Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Input, Dense, Dropout\n",
        "import numpy as np\n",
        "\n",
        "# Step 1: Load training data\n",
        "train_df = pd.read_csv('train.csv')\n",
        "\n",
        "# Step 2: Select feature columns and target\n",
        "features = [\n",
        "    'LotArea', 'OverallQual', 'OverallCond', 'YearBuilt', 'YearRemodAdd',\n",
        "    'TotalBsmtSF', 'GrLivArea', 'FullBath', 'HalfBath',\n",
        "    'BedroomAbvGr', 'TotRmsAbvGrd', 'GarageCars', 'GarageArea'\n",
        "]\n",
        "target = 'SalePrice'\n",
        "\n",
        "# Step 3: Handle missing values in features\n",
        "X = train_df[features].fillna(train_df[features].mean())\n",
        "y = train_df[target]\n",
        "\n",
        "# Step 4: Split into training and validation sets\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Step 5: Scale features\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_val = scaler.transform(X_val)\n",
        "\n",
        "# Step 6: Build the MLP model with Input Layer\n",
        "model = Sequential([\n",
        "    Input(shape=(X_train.shape[1],)),\n",
        "    Dense(128, activation='relu'),\n",
        "    Dropout(0.2),\n",
        "    Dense(64, activation='relu'),\n",
        "    Dropout(0.1),\n",
        "    Dense(32, activation='relu'),\n",
        "    Dense(1)\n",
        "])\n",
        "\n",
        "model.compile(optimizer='adam', loss='mean_squared_error', metrics=['mae'])\n",
        "\n",
        "# Step 7: Train the model\n",
        "history = model.fit(X_train, y_train, epochs=100, batch_size=32, validation_data=(X_val, y_val))\n",
        "\n",
        "# Step 8: Predict on validation set\n",
        "y_val_pred = model.predict(X_val).flatten()\n",
        "\n",
        "# Step 9: Calculate accuracy metrics on validation set\n",
        "mae = mean_absolute_error(y_val, y_val_pred)\n",
        "rmse = np.sqrt(mean_squared_error(y_val, y_val_pred))\n",
        "\n",
        "print(f'Validation MAE: {mae:.2f}')\n",
        "print(f'Validation RMSE: {rmse:.2f}')\n",
        "\n",
        "# Step 10: Show some sample predictions vs actual\n",
        "print(\"\\nSample predictions on validation data:\")\n",
        "for i in range(5):\n",
        "    print(f\"Actual: {y_val.iloc[i]}, Predicted: {y_val_pred[i]:.2f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iIzfQpRjrS2G",
        "outputId": "70284375-aa9e-45dd-90d7-c9bfb4d1e554"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 86ms/step - loss: 39191998464.0000 - mae: 182079.9844 - val_loss: 39650557952.0000 - val_mae: 178831.0625\n",
            "Epoch 2/100\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 38278713344.0000 - mae: 180404.2031 - val_loss: 39616786432.0000 - val_mae: 178750.8750\n",
            "Epoch 3/100\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 39959748608.0000 - mae: 183397.5781 - val_loss: 39437565952.0000 - val_mae: 178339.8125\n",
            "Epoch 4/100\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 38213132288.0000 - mae: 180744.3125 - val_loss: 38836162560.0000 - val_mae: 176977.1875\n",
            "Epoch 5/100\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 36247748608.0000 - mae: 176157.6406 - val_loss: 37359063040.0000 - val_mae: 173605.2656\n",
            "Epoch 6/100\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 35728998400.0000 - mae: 173987.9688 - val_loss: 34400448512.0000 - val_mae: 166676.5000\n",
            "Epoch 7/100\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 33219911680.0000 - mae: 166770.2500 - val_loss: 29555298304.0000 - val_mae: 154552.4062\n",
            "Epoch 8/100\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 26805790720.0000 - mae: 151811.7344 - val_loss: 23033835520.0000 - val_mae: 136280.4844\n",
            "Epoch 9/100\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 21254490112.0000 - mae: 135187.8750 - val_loss: 15781931008.0000 - val_mae: 112130.9375\n",
            "Epoch 10/100\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 15324440576.0000 - mae: 111574.0859 - val_loss: 9807073280.0000 - val_mae: 86769.6797\n",
            "Epoch 11/100\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 9652745216.0000 - mae: 85035.7109 - val_loss: 6650982912.0000 - val_mae: 68714.1484\n",
            "Epoch 12/100\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 7313755136.0000 - mae: 68525.9766 - val_loss: 5417336832.0000 - val_mae: 59772.6562\n",
            "Epoch 13/100\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 6269161984.0000 - mae: 62878.7656 - val_loss: 5033882112.0000 - val_mae: 56661.1758\n",
            "Epoch 14/100\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 5572029952.0000 - mae: 59820.5156 - val_loss: 4801722880.0000 - val_mae: 54964.1680\n",
            "Epoch 15/100\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 6089911808.0000 - mae: 58672.4883 - val_loss: 4628212224.0000 - val_mae: 53866.1914\n",
            "Epoch 16/100\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 5743217152.0000 - mae: 58654.9219 - val_loss: 4446224384.0000 - val_mae: 52737.3398\n",
            "Epoch 17/100\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 6567706624.0000 - mae: 58981.6562 - val_loss: 4293515520.0000 - val_mae: 51763.0859\n",
            "Epoch 18/100\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 4937620480.0000 - mae: 55712.6562 - val_loss: 4099960832.0000 - val_mae: 50435.3633\n",
            "Epoch 19/100\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 4950543360.0000 - mae: 54220.3164 - val_loss: 3952911104.0000 - val_mae: 49496.5156\n",
            "Epoch 20/100\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 4118141440.0000 - mae: 50621.3750 - val_loss: 3795168768.0000 - val_mae: 48444.9414\n",
            "Epoch 21/100\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 5358065152.0000 - mae: 53662.8047 - val_loss: 3638822912.0000 - val_mae: 47411.8438\n",
            "Epoch 22/100\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 3935423232.0000 - mae: 50300.7266 - val_loss: 3479012864.0000 - val_mae: 46251.3203\n",
            "Epoch 23/100\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 5509391872.0000 - mae: 52471.2695 - val_loss: 3346899968.0000 - val_mae: 45467.8086\n",
            "Epoch 24/100\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 3949213696.0000 - mae: 48832.8047 - val_loss: 3197674752.0000 - val_mae: 44285.6992\n",
            "Epoch 25/100\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 3793714944.0000 - mae: 46407.7227 - val_loss: 3103694848.0000 - val_mae: 43804.0156\n",
            "Epoch 26/100\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 3743478528.0000 - mae: 45439.7852 - val_loss: 2977000448.0000 - val_mae: 42729.5156\n",
            "Epoch 27/100\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - loss: 3351172096.0000 - mae: 44573.2695 - val_loss: 2858542336.0000 - val_mae: 41890.0352\n",
            "Epoch 28/100\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 3412840192.0000 - mae: 43720.0547 - val_loss: 2741987072.0000 - val_mae: 40889.4375\n",
            "Epoch 29/100\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 3058425088.0000 - mae: 42013.2695 - val_loss: 2619634944.0000 - val_mae: 39854.9180\n",
            "Epoch 30/100\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 2845255680.0000 - mae: 40693.1602 - val_loss: 2520647680.0000 - val_mae: 39005.6875\n",
            "Epoch 31/100\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 3203060480.0000 - mae: 41422.0391 - val_loss: 2427362816.0000 - val_mae: 38193.2422\n",
            "Epoch 32/100\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 3580215040.0000 - mae: 40455.0547 - val_loss: 2343905536.0000 - val_mae: 37357.3984\n",
            "Epoch 33/100\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 2892014592.0000 - mae: 38837.8047 - val_loss: 2271172864.0000 - val_mae: 36745.6602\n",
            "Epoch 34/100\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 2551674624.0000 - mae: 38097.7539 - val_loss: 2215581696.0000 - val_mae: 36060.8750\n",
            "Epoch 35/100\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 2547084032.0000 - mae: 36819.6719 - val_loss: 2151472128.0000 - val_mae: 35358.2109\n",
            "Epoch 36/100\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 2295207424.0000 - mae: 35692.7344 - val_loss: 2053607808.0000 - val_mae: 34364.3320\n",
            "Epoch 37/100\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 2117042816.0000 - mae: 34240.4531 - val_loss: 1977950080.0000 - val_mae: 33602.7969\n",
            "Epoch 38/100\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 2133231232.0000 - mae: 35056.2461 - val_loss: 1942337152.0000 - val_mae: 33144.6797\n",
            "Epoch 39/100\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 2326481152.0000 - mae: 34405.6367 - val_loss: 1886891392.0000 - val_mae: 32525.2539\n",
            "Epoch 40/100\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 2155453952.0000 - mae: 33922.1289 - val_loss: 1867376512.0000 - val_mae: 32186.4004\n",
            "Epoch 41/100\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1934380160.0000 - mae: 34090.5430 - val_loss: 1809632768.0000 - val_mae: 31586.6035\n",
            "Epoch 42/100\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 2265010176.0000 - mae: 33561.5781 - val_loss: 1801442304.0000 - val_mae: 31309.9043\n",
            "Epoch 43/100\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1965251328.0000 - mae: 31762.2383 - val_loss: 1758023936.0000 - val_mae: 30818.0352\n",
            "Epoch 44/100\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 2102106496.0000 - mae: 32290.4648 - val_loss: 1738375680.0000 - val_mae: 30480.5586\n",
            "Epoch 45/100\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1895857536.0000 - mae: 32909.5117 - val_loss: 1670494848.0000 - val_mae: 29825.8867\n",
            "Epoch 46/100\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1959795840.0000 - mae: 31260.9766 - val_loss: 1684070016.0000 - val_mae: 29788.4102\n",
            "Epoch 47/100\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 2275310336.0000 - mae: 32954.2891 - val_loss: 1642996864.0000 - val_mae: 29314.3730\n",
            "Epoch 48/100\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 2024986752.0000 - mae: 30465.2480 - val_loss: 1625591680.0000 - val_mae: 29006.3555\n",
            "Epoch 49/100\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1759352064.0000 - mae: 29796.3379 - val_loss: 1600016256.0000 - val_mae: 28674.4727\n",
            "Epoch 50/100\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 2354697216.0000 - mae: 30927.4863 - val_loss: 1586625536.0000 - val_mae: 28431.1777\n",
            "Epoch 51/100\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1908513024.0000 - mae: 31177.5508 - val_loss: 1556862976.0000 - val_mae: 28109.8945\n",
            "Epoch 52/100\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1843176832.0000 - mae: 29275.5938 - val_loss: 1530408576.0000 - val_mae: 27844.0195\n",
            "Epoch 53/100\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 2433864192.0000 - mae: 31405.5918 - val_loss: 1543612928.0000 - val_mae: 27822.0586\n",
            "Epoch 54/100\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 2028840960.0000 - mae: 30966.8477 - val_loss: 1503057152.0000 - val_mae: 27471.0664\n",
            "Epoch 55/100\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1942683008.0000 - mae: 30616.9434 - val_loss: 1506779264.0000 - val_mae: 27415.4688\n",
            "Epoch 56/100\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1371491968.0000 - mae: 27458.6387 - val_loss: 1473785728.0000 - val_mae: 27067.9082\n",
            "Epoch 57/100\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1561507840.0000 - mae: 27851.1934 - val_loss: 1501977600.0000 - val_mae: 27112.4570\n",
            "Epoch 58/100\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1705520640.0000 - mae: 29509.8301 - val_loss: 1454803328.0000 - val_mae: 26737.1738\n",
            "Epoch 59/100\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1512418816.0000 - mae: 26873.5430 - val_loss: 1460763136.0000 - val_mae: 26665.2500\n",
            "Epoch 60/100\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1769947136.0000 - mae: 28243.0664 - val_loss: 1461955840.0000 - val_mae: 26605.3633\n",
            "Epoch 61/100\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1782608384.0000 - mae: 29651.1699 - val_loss: 1458935936.0000 - val_mae: 26456.9805\n",
            "Epoch 62/100\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1630525568.0000 - mae: 27202.8613 - val_loss: 1453112704.0000 - val_mae: 26334.1328\n",
            "Epoch 63/100\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1584010880.0000 - mae: 27536.3457 - val_loss: 1412023040.0000 - val_mae: 26018.2520\n",
            "Epoch 64/100\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1697208832.0000 - mae: 27576.0410 - val_loss: 1416515456.0000 - val_mae: 25970.4160\n",
            "Epoch 65/100\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1975044352.0000 - mae: 27790.5293 - val_loss: 1439430656.0000 - val_mae: 25999.4980\n",
            "Epoch 66/100\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1767774080.0000 - mae: 28399.3672 - val_loss: 1419227904.0000 - val_mae: 25827.3613\n",
            "Epoch 67/100\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1222762496.0000 - mae: 25630.1719 - val_loss: 1402585600.0000 - val_mae: 25616.5391\n",
            "Epoch 68/100\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1723303296.0000 - mae: 27353.8516 - val_loss: 1407582592.0000 - val_mae: 25584.8945\n",
            "Epoch 69/100\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1459608704.0000 - mae: 27320.6641 - val_loss: 1392871168.0000 - val_mae: 25469.8145\n",
            "Epoch 70/100\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 2168625920.0000 - mae: 28844.3203 - val_loss: 1386651392.0000 - val_mae: 25383.2422\n",
            "Epoch 71/100\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1332620544.0000 - mae: 26095.2949 - val_loss: 1373029376.0000 - val_mae: 25253.4785\n",
            "Epoch 72/100\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1858659840.0000 - mae: 26773.6113 - val_loss: 1395788032.0000 - val_mae: 25253.5957\n",
            "Epoch 73/100\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1248859008.0000 - mae: 25679.5000 - val_loss: 1392984192.0000 - val_mae: 25137.3125\n",
            "Epoch 74/100\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1870326528.0000 - mae: 26884.1406 - val_loss: 1385807104.0000 - val_mae: 25064.0586\n",
            "Epoch 75/100\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1406756224.0000 - mae: 26951.7266 - val_loss: 1352090880.0000 - val_mae: 24880.1641\n",
            "Epoch 76/100\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1537650560.0000 - mae: 25667.4707 - val_loss: 1368709760.0000 - val_mae: 24929.6758\n",
            "Epoch 77/100\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1662185728.0000 - mae: 26696.8379 - val_loss: 1341027200.0000 - val_mae: 24785.1504\n",
            "Epoch 78/100\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1948469120.0000 - mae: 28392.7480 - val_loss: 1351577216.0000 - val_mae: 24714.1992\n",
            "Epoch 79/100\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1548733312.0000 - mae: 26425.1367 - val_loss: 1383691264.0000 - val_mae: 24766.1328\n",
            "Epoch 80/100\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1406846848.0000 - mae: 25521.6289 - val_loss: 1347660032.0000 - val_mae: 24633.7852\n",
            "Epoch 81/100\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 1584994560.0000 - mae: 26933.6953 - val_loss: 1351051392.0000 - val_mae: 24542.7773\n",
            "Epoch 82/100\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 1388841856.0000 - mae: 25679.1875 - val_loss: 1347535616.0000 - val_mae: 24481.5605\n",
            "Epoch 83/100\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1706250368.0000 - mae: 26084.3066 - val_loss: 1326291072.0000 - val_mae: 24417.3223\n",
            "Epoch 84/100\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1662736768.0000 - mae: 25679.7812 - val_loss: 1328036864.0000 - val_mae: 24333.3125\n",
            "Epoch 85/100\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1641061376.0000 - mae: 25414.9980 - val_loss: 1361009664.0000 - val_mae: 24407.1426\n",
            "Epoch 86/100\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1514526336.0000 - mae: 26169.3164 - val_loss: 1339885824.0000 - val_mae: 24245.9238\n",
            "Epoch 87/100\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1487799296.0000 - mae: 25159.6016 - val_loss: 1343432064.0000 - val_mae: 24261.5605\n",
            "Epoch 88/100\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1475097728.0000 - mae: 25474.8125 - val_loss: 1334348032.0000 - val_mae: 24223.4043\n",
            "Epoch 89/100\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1344272384.0000 - mae: 24761.1895 - val_loss: 1339713536.0000 - val_mae: 24195.1758\n",
            "Epoch 90/100\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1362155264.0000 - mae: 25423.8691 - val_loss: 1294666880.0000 - val_mae: 24081.5469\n",
            "Epoch 91/100\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1426798976.0000 - mae: 25789.4961 - val_loss: 1303239168.0000 - val_mae: 23999.7656\n",
            "Epoch 92/100\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1941570816.0000 - mae: 25928.4316 - val_loss: 1295289472.0000 - val_mae: 23965.9824\n",
            "Epoch 93/100\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1594824832.0000 - mae: 25554.7852 - val_loss: 1306156928.0000 - val_mae: 23954.1426\n",
            "Epoch 94/100\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1422923136.0000 - mae: 24562.8652 - val_loss: 1326659200.0000 - val_mae: 23984.2773\n",
            "Epoch 95/100\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1429498624.0000 - mae: 24434.6191 - val_loss: 1333504128.0000 - val_mae: 24000.6973\n",
            "Epoch 96/100\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1178803584.0000 - mae: 24413.8398 - val_loss: 1316156288.0000 - val_mae: 23885.7695\n",
            "Epoch 97/100\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1129147008.0000 - mae: 23879.2246 - val_loss: 1296452224.0000 - val_mae: 23802.2539\n",
            "Epoch 98/100\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1592904064.0000 - mae: 25840.9512 - val_loss: 1308083584.0000 - val_mae: 23794.8672\n",
            "Epoch 99/100\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1378130048.0000 - mae: 25277.2285 - val_loss: 1266546432.0000 - val_mae: 23661.0176\n",
            "Epoch 100/100\n",
            "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1231337088.0000 - mae: 23661.9004 - val_loss: 1362235904.0000 - val_mae: 23981.6602\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step\n",
            "Validation MAE: 23981.66\n",
            "Validation RMSE: 36908.48\n",
            "\n",
            "Sample predictions on validation data:\n",
            "Actual: 154500, Predicted: 149744.00\n",
            "Actual: 325000, Predicted: 278818.84\n",
            "Actual: 115000, Predicted: 127043.45\n",
            "Actual: 159000, Predicted: 151774.67\n",
            "Actual: 315500, Predicted: 309279.69\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Function to predict price from new input ---\n",
        "def predict_price(input_dict):\n",
        "    input_df = pd.DataFrame([input_dict])\n",
        "    for feat in features:\n",
        "        if feat not in input_df.columns:\n",
        "            input_df[feat] = train_df[feat].mean()\n",
        "    input_df = input_df[features]\n",
        "    input_df = input_df.fillna(train_df[features].mean())\n",
        "    input_scaled = scaler.transform(input_df)\n",
        "    pred_array = model.predict(input_scaled)\n",
        "    pred_price = pred_array[0, 0]  # Extract scalar properly\n",
        "    return pred_price\n",
        "\n",
        "# Example usage of prediction function\n",
        "example_input = {\n",
        "    'LotArea': 8450,\n",
        "    'OverallQual': 7,\n",
        "    'OverallCond': 5,\n",
        "    'YearBuilt': 2003,\n",
        "    'YearRemodAdd': 2003,\n",
        "    'TotalBsmtSF': 856,\n",
        "    'GrLivArea': 1710,\n",
        "    'FullBath': 2,\n",
        "    'HalfBath': 1,\n",
        "    'BedroomAbvGr': 3,\n",
        "    'TotRmsAbvGrd': 8,\n",
        "    'GarageCars': 2,\n",
        "    'GarageArea': 548\n",
        "}\n",
        "\n",
        "predicted_price = predict_price(example_input)\n",
        "print(f\"\\nPredicted house price for example input: ${predicted_price:.2f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ROxrYTnlr-2L",
        "outputId": "108593a8-2f5a-4bbb-dc39-56302868e558"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 205ms/step\n",
            "\n",
            "Predicted house price for example input: $208654.31\n"
          ]
        }
      ]
    }
  ]
}