{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMMz+TSJ5EyV8x3dCJ3teDL",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/korupolujayanth2004/DeepLearning/blob/main/Perceptron_Logic_Gates.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5k93AvidgGeU",
        "outputId": "61ab256d-0f7c-4da8-be62-140fc7fb5645"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "AND Gate:\n",
            "0 0 -> 0\n",
            "0 1 -> 0\n",
            "1 0 -> 0\n",
            "1 1 -> 1\n",
            "OR Gate:\n",
            "0 0 -> 0\n",
            "0 1 -> 1\n",
            "1 0 -> 1\n",
            "1 1 -> 1\n",
            "NOT Gate:\n",
            "0 -> 1\n",
            "1 -> 0\n"
          ]
        }
      ],
      "source": [
        "def perceptron(inputs,weights,bias):\n",
        "  output=sum(w*x for w,x in zip(weights,inputs))+bias\n",
        "  return 1 if output>=0 else 0\n",
        "\n",
        "def and_gate(x1,x2):\n",
        "  return perceptron([x1,x2],weights=[1,1],bias=-1.5)\n",
        "\n",
        "def or_gate(x1,x2):\n",
        "  return perceptron([x1,x2],weights=[1,1],bias=-0.5)\n",
        "\n",
        "def not_gate(x1):\n",
        "  return perceptron([x1],weights=[-1],bias=0.5)\n",
        "\n",
        "#And Gate\n",
        "print(\"AND Gate:\")\n",
        "print(\"0 0 ->\", and_gate(0,0))\n",
        "print(\"0 1 ->\",and_gate(0,1))\n",
        "print(\"1 0 ->\",and_gate(1,0))\n",
        "print(\"1 1 ->\",and_gate(1,1))\n",
        "\n",
        "#Or_Gate\n",
        "print(\"OR Gate:\")\n",
        "print(\"0 0 ->\", or_gate(0,0))\n",
        "print(\"0 1 ->\",or_gate(0,1))\n",
        "print(\"1 0 ->\",or_gate(1,0))\n",
        "print(\"1 1 ->\",or_gate(1,1))\n",
        "\n",
        "#NotGate\n",
        "print(\"NOT Gate:\")\n",
        "print(\"0 ->\",not_gate(0))\n",
        "print(\"1 ->\",not_gate(1))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def train_perceptron(training_data, learning_rate, epochs):\n",
        "    \"\"\"\n",
        "    Trains a single-layer perceptron using the Perceptron Learning Rule.\n",
        "\n",
        "    Args:\n",
        "        training_data (list of tuples): Each tuple is (inputs, desired_output).\n",
        "                                        e.g., [([0,0], 0), ([0,1], 0), ([1,0], 0), ([1,1], 1)]\n",
        "        learning_rate (float): The step size for updating weights and bias.\n",
        "        epochs (int): The number of times to iterate over the entire training data.\n",
        "\n",
        "    Returns:\n",
        "        tuple: (weights, bias) of the trained perceptron.\n",
        "    \"\"\"\n",
        "    # Initialize weights and bias randomly, typically small values around 0.\n",
        "    # For a 2-input gate, we need 2 weights.\n",
        "    weights = np.random.rand(len(training_data[0][0])) * 0.1 # Small random weights\n",
        "    bias = np.random.rand() * 0.1 # Small random bias\n",
        "\n",
        "    print(f\"Initial Weights: {weights}, Initial Bias: {bias:.4f}\\n\")\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        total_error = 0\n",
        "        for inputs, desired_output in training_data:\n",
        "            # 1. Forward Pass: Calculate the weighted sum\n",
        "            weighted_sum = np.dot(inputs, weights) + bias\n",
        "\n",
        "            # 2. Activation Function (Binary Step)\n",
        "            predicted_output = 1 if weighted_sum >= 0 else 0\n",
        "\n",
        "            # 3. Calculate Error\n",
        "            error = desired_output - predicted_output\n",
        "            total_error += abs(error) # Sum of absolute errors for monitoring\n",
        "\n",
        "            # 4. Update Weights and Bias (Perceptron Learning Rule)\n",
        "            # weights_new = weights_old + learning_rate * error * input_value\n",
        "            # bias_new = bias_old + learning_rate * error * 1 (since bias input is always 1)\n",
        "            weights += learning_rate * error * np.array(inputs)\n",
        "            bias += learning_rate * error\n",
        "\n",
        "        # Print progress every few epochs or if the error is 0\n",
        "        if (epoch + 1) % 100 == 0 or total_error == 0:\n",
        "            print(f\"Epoch {epoch + 1}/{epochs}, Total Error: {total_error}\")\n",
        "            print(f\"  Current Weights: {weights}, Current Bias: {bias:.4f}\\n\")\n",
        "\n",
        "        # If there's no error, the perceptron has learned the pattern, so we can stop\n",
        "        if total_error == 0:\n",
        "            print(f\"Converged at Epoch {epoch + 1}\")\n",
        "            break\n",
        "\n",
        "    print(\"\\n--- Training Complete ---\")\n",
        "    print(f\"Final Weights: {weights}\")\n",
        "    print(f\"Final Bias: {bias:.4f}\")\n",
        "    return weights, bias\n",
        "\n",
        "def test_perceptron(inputs, weights, bias):\n",
        "    \"\"\"\n",
        "    Tests the trained perceptron with new inputs.\n",
        "    \"\"\"\n",
        "    weighted_sum = np.dot(inputs, weights) + bias\n",
        "    return 1 if weighted_sum >= 0 else 0\n",
        "\n",
        "# --- Training for AND Gate ---\n",
        "print(\"--- Training Perceptron for AND Gate ---\")\n",
        "and_training_data = [\n",
        "    ([0, 0], 0),\n",
        "    ([0, 1], 0),\n",
        "    ([1, 0], 0),\n",
        "    ([1, 1], 1)\n",
        "]\n",
        "\n",
        "# Hyperparameters\n",
        "learning_rate_and = 0.1\n",
        "epochs_and = 1000\n",
        "\n",
        "# Train the perceptron for the AND gate\n",
        "learned_weights_and, learned_bias_and = train_perceptron(and_training_data, learning_rate_and, epochs_and)\n",
        "\n",
        "# --- Testing the Trained AND Gate ---\n",
        "print(\"\\n--- Testing Learned AND Gate ---\")\n",
        "print(f\"0 AND 0 -> {test_perceptron([0,0], learned_weights_and, learned_bias_and)}\")\n",
        "print(f\"0 AND 1 -> {test_perceptron([0,1], learned_weights_and, learned_bias_and)}\")\n",
        "print(f\"1 AND 0 -> {test_perceptron([1,0], learned_weights_and, learned_bias_and)}\")\n",
        "print(f\"1 AND 1 -> {test_perceptron([1,1], learned_weights_and, learned_bias_and)}\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*40 + \"\\n\")\n",
        "\n",
        "# --- Training for OR Gate ---\n",
        "print(\"--- Training Perceptron for OR Gate ---\")\n",
        "or_training_data = [\n",
        "    ([0, 0], 0),\n",
        "    ([0, 1], 1),\n",
        "    ([1, 0], 1),\n",
        "    ([1, 1], 1)\n",
        "]\n",
        "\n",
        "# Hyperparameters\n",
        "learning_rate_or = 0.1\n",
        "epochs_or = 1000\n",
        "\n",
        "# Train the perceptron for the OR gate\n",
        "learned_weights_or, learned_bias_or = train_perceptron(or_training_data, learning_rate_or, epochs_or)\n",
        "\n",
        "# --- Testing the Trained OR Gate ---\n",
        "print(\"\\n--- Testing Learned OR Gate ---\")\n",
        "print(f\"0 OR 0 -> {test_perceptron([0,0], learned_weights_or, learned_bias_or)}\")\n",
        "print(f\"0 OR 1 -> {test_perceptron([0,1], learned_weights_or, learned_bias_or)}\")\n",
        "print(f\"1 OR 0 -> {test_perceptron([1,0], learned_weights_or, learned_bias_or)}\")\n",
        "print(f\"1 OR 1 -> {test_perceptron([1,1], learned_weights_or, learned_bias_or)}\")\n",
        "\n",
        "\n",
        "print(\"\\n\" + \"=\"*40 + \"\\n\")\n",
        "\n",
        "# --- Training for NOT Gate ---\n",
        "print(\"--- Training Perceptron for NOT Gate ---\")\n",
        "not_training_data = [\n",
        "    ([0], 1), # NOT 0 is 1\n",
        "    ([1], 0)  # NOT 1 is 0\n",
        "]\n",
        "\n",
        "# Hyperparameters\n",
        "learning_rate_not = 0.1\n",
        "epochs_not = 1000\n",
        "\n",
        "# Train the perceptron for the NOT gate\n",
        "learned_weights_not, learned_bias_not = train_perceptron(not_training_data, learning_rate_not, epochs_not)\n",
        "\n",
        "# --- Testing the Trained NOT Gate ---\n",
        "print(\"\\n--- Testing Learned NOT Gate ---\")\n",
        "print(f\"NOT 0 -> {test_perceptron([0], learned_weights_not, learned_bias_not)}\")\n",
        "print(f\"NOT 1 -> {test_perceptron([1], learned_weights_not, learned_bias_not)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T3BpJX_HhCXO",
        "outputId": "5b49291e-75dc-4898-eaee-5b42f344ca46"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Training Perceptron for AND Gate ---\n",
            "Initial Weights: [0.08283614 0.04847334], Initial Bias: 0.0424\n",
            "\n",
            "Epoch 6/1000, Total Error: 0\n",
            "  Current Weights: [0.18283614 0.14847334], Current Bias: -0.2576\n",
            "\n",
            "Converged at Epoch 6\n",
            "\n",
            "--- Training Complete ---\n",
            "Final Weights: [0.18283614 0.14847334]\n",
            "Final Bias: -0.2576\n",
            "\n",
            "--- Testing Learned AND Gate ---\n",
            "0 AND 0 -> 0\n",
            "0 AND 1 -> 0\n",
            "1 AND 0 -> 0\n",
            "1 AND 1 -> 1\n",
            "\n",
            "========================================\n",
            "\n",
            "--- Training Perceptron for OR Gate ---\n",
            "Initial Weights: [0.00827421 0.0067637 ], Initial Bias: 0.0842\n",
            "\n",
            "Epoch 4/1000, Total Error: 0\n",
            "  Current Weights: [0.10827421 0.1067637 ], Current Bias: -0.0158\n",
            "\n",
            "Converged at Epoch 4\n",
            "\n",
            "--- Training Complete ---\n",
            "Final Weights: [0.10827421 0.1067637 ]\n",
            "Final Bias: -0.0158\n",
            "\n",
            "--- Testing Learned OR Gate ---\n",
            "0 OR 0 -> 0\n",
            "0 OR 1 -> 1\n",
            "1 OR 0 -> 1\n",
            "1 OR 1 -> 1\n",
            "\n",
            "========================================\n",
            "\n",
            "--- Training Perceptron for NOT Gate ---\n",
            "Initial Weights: [0.04456469], Initial Bias: 0.0970\n",
            "\n",
            "Epoch 4/1000, Total Error: 0\n",
            "  Current Weights: [-0.15543531], Current Bias: 0.0970\n",
            "\n",
            "Converged at Epoch 4\n",
            "\n",
            "--- Training Complete ---\n",
            "Final Weights: [-0.15543531]\n",
            "Final Bias: 0.0970\n",
            "\n",
            "--- Testing Learned NOT Gate ---\n",
            "NOT 0 -> 1\n",
            "NOT 1 -> 0\n"
          ]
        }
      ]
    }
  ]
}